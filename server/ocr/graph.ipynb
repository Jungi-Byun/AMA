{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ae8027-1b60-4ae2-882c-168d161bc18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install easyocr transformers torch\n",
    "!pip install requests opencv-python\n",
    "!pip install opencv-python-headless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e58dc8-3cf0-4c0f-9fc9-f2f398a8bf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade git+https://github.com/huggingface/transformers\n",
    "!pip install accelerate\n",
    "!pip install timm==1.0.13\n",
    "!pip install --upgrade accelerate transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb796b4-d6eb-4203-9209-be9c08ff633f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install langgraph\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9149d8-accb-48c9-a549-f4d04df3a614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import requests\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc234d2-6d79-4dbe-bdbe-bbca7f937f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import pickle\n",
    "import editdistance\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ac74a4-eb0e-477b-b30c-070a94e5e5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from typing import TypedDict, Any, Dict, List\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0a06ea-e7ef-455f-9c26-31c6c3090bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from paddleocr import PaddleOCR\n",
    "import easyocr\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d2bb62-74b5-4e37-a445-2797a68f919f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad5c3ec7-23ab-4cd7-8dea-22c0676760cc",
   "metadata": {},
   "source": [
    "### LangGraph êµ¬ì„±\n",
    "OCR output â†’ LLM â†’ ìš”ì•½ë¬¸ â†’ VectorDB search â†’ ìš”ì•½ë¬¸ ê´€ë ¨ ë¬¸ì œ â†’ LLM â†’ íŒíŠ¸ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc88ad4-72ef-4122-85b8-8af35cf36085",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# curriculum_units.json íŒŒì¼ì„ ì½ì–´ì„œ ë³€ìˆ˜ë¡œ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "with open(\"curriculum_units.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    curriculum_units = json.load(f)\n",
    "\n",
    "curriculum_titles = \"\\n\".join(item[\"title\"] for item in curriculum_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842a966a-391e-49bb-b127-48b93bf3c8ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## ì„¤ëª…ì´ ì¶”ê°€ëœ ì†Œë‹¨ì› ë‚´ìš©\n",
    "# curriculum_units = loaded_subtopics\n",
    "# curriculum_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70aae2c8-561c-4425-bc61-553cbab2f4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llM ëª¨ë¸ ì¤€ë¹„\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct\"\n",
    "\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dabeab-8373-4839-bc4e-99d6bb11a79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# small_model_name = \"LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct\"\n",
    "\n",
    "# small_llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "#     small_model_name,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     trust_remote_code=True,\n",
    "#     device_map=\"auto\"\n",
    "# )\n",
    "# small_tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a29b9a-d3b7-4b69-a722-392acfdf82a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    # trocr_result: str\n",
    "    easyocr_result: str\n",
    "    paddleocr_result: str\n",
    "    merged_text: str # OCR resultë“¤ì„ reconstructioní•œ ê²°ê³¼\n",
    "    summary: str # LLM ìš”ì•½ë¬¸\n",
    "    is_math_related: bool # ìˆ˜í•™ ê´€ë ¨ ì—¬ë¶€\n",
    "    warning: str\n",
    "    valid_topic: str # ìµœì¢… ì„ íƒ ë‹¨ì›"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b106ae6-6907-485c-aa6d-02a8f7dd4d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_bool(value):\n",
    "    return str(value).strip().lower() == \"yes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52545da0-4370-4293-a019-f0c1c6e4c71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ocr_merge(easy, paddle) -> str:\n",
    "    prompt = f\"\"\"ë‹¤ìŒì€ ë™ì¼í•œ êµì•ˆ ì´ë¯¸ì§€ì— ëŒ€í•´ ë‘ ê°€ì§€ OCR ì—”ì§„(EasyOCR, PaddleOCR)ì´ ì¶”ì¶œí•œ í…ìŠ¤íŠ¸ ê²°ê³¼ì…ë‹ˆë‹¤.  \n",
    "ë‘ ê²°ê³¼ë¥¼ ë¹„êµí•˜ì—¬ ì˜ëª»ëœ ë¬¸ì¥ì„ ë³´ì™„í•˜ê³ , ê°€ëŠ¥í•œ í•œ ì •í™•í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ í•˜ë‚˜ì˜ ìµœì¢… ê²°ê³¼ë¡œ í•©ì³ì£¼ì„¸ìš”.  \n",
    "ë¬¸ë§¥ìƒ ë§ì§€ ì•Šê±°ë‚˜ ì¸ì‹ì´ ì˜ëª»ëœ ë‹¨ì–´ëŠ” ìœ ì¶”í•˜ì—¬ ìˆ˜ì •í•´ ì£¼ì„¸ìš”.  \n",
    "ìµœì¢… ê²°ê³¼ í…ìŠ¤íŠ¸ë¥¼ ë¨¼ì € ì¶œë ¥í•˜ê³ , ë§ˆì§€ë§‰ì— ìµœì¢… ê²°ê³¼ê°€ ìˆ˜í•™ êµìœ¡ê³¼ ê´€ë ¨ëœ ë‚´ìš©ì¸ì§€ ì—¬ë¶€ë¥¼ ì•„ë˜ì™€ ê°™ì€ JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•´ ì£¼ì„¸ìš”.\n",
    "\n",
    "```json\n",
    "{{\"is_math_related\": \"yes\"}}  # ë˜ëŠ” {{\"is_math_related\": \"no\"}}\n",
    "\n",
    "### EasyOCR ê²°ê³¼:\n",
    "{easy}\n",
    "\n",
    "### PaddleOCR ê²°ê³¼:\n",
    "{paddle}\n",
    "\"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \n",
    "         \"content\":\"ë‹¹ì‹ ì€ ë‘ ê°œì˜ OCR ê²°ê³¼(EasyOCRì™€ PaddleOCR)ë¥¼ ë¹„êµí•˜ì—¬, \"\n",
    "                   \"ìµœëŒ€í•œ ì •í™•í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ í•˜ë‚˜ì˜ í†µí•©ëœ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ì „ë¬¸ AIì…ë‹ˆë‹¤.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    output_ids = llm_model.generate(\n",
    "        input_ids.to(\"cuda\"),\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=1024,\n",
    "        do_sample=False,\n",
    "    )\n",
    "\n",
    "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    result = output_text.split(\"[|assistant|]\")[-1].strip()\n",
    "\n",
    "    # âœ… is_math_related ê°’ ì¶”ì¶œ\n",
    "    match = re.search(r'```json\\s*(\\{.*?\\})\\s*```', result, re.DOTALL)\n",
    "    if match:\n",
    "        try:\n",
    "            is_math_related_str = json.loads(match.group(1))[\"is_math_related\"]\n",
    "            is_math_related = is_math_related_str.lower() == \"yes\"\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"JSON íŒŒì‹± ì˜¤ë¥˜: {e}\")\n",
    "    else:\n",
    "        raise ValueError(\"âš ï¸ JSON ë¸”ë¡ì„ ê²°ê³¼ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    # âœ… JSON ë¸”ë¡ ì œê±°í•œ ìµœì¢… merged_text\n",
    "    merged_text = re.sub(r'```json\\s*\\{.*?\\}\\s*```', '', result, flags=re.DOTALL).strip()\n",
    "\n",
    "    # JSON í—¤ë” ë‹¨ë… ì œê±°\n",
    "    merged_text = re.sub(r'### JSON ê²°ê³¼:?\\s*', '', merged_text).strip()\n",
    "\n",
    "    # âœ… ì¶œë ¥ ë¡œê·¸ (í•„ìš” ì‹œ ì œê±° ê°€ëŠ¥)\n",
    "    print(\"\\nâœ… í†µí•© í…ìŠ¤íŠ¸:\\n\", merged_text)\n",
    "    print(\"\\nâœ… ìˆ˜í•™ ê´€ë ¨ ì—¬ë¶€:\", is_math_related_str)\n",
    "\n",
    "    # âœ… ë°˜í™˜\n",
    "    if not is_math_related:\n",
    "        return {\n",
    "            \"merged_text\": merged_text,\n",
    "            \"is_math_related\": False,\n",
    "            \"warning\": \"âš ï¸ ìˆ˜í•™ ê´€ë ¨ ë‚´ìš©ì´ ì•„ë‹™ë‹ˆë‹¤. ê·¸ë˜í”„ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.\"\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"merged_text\": merged_text,\n",
    "            \"is_math_related\": True\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63db41e-e3b5-4bee-a40c-7e833f086225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_exaone_summary(text: str) -> str:\n",
    "    prompt = f\"\"\"ë‹¹ì‹ ì€ ì´ˆë“±í•™êµ ìˆ˜í•™ êµìœ¡ ìë£Œë¥¼ ë¶„ì„í•˜ê³  ë¶„ë¥˜í•˜ëŠ” ìœ ëŠ¥í•œ AIì…ë‹ˆë‹¤.  \n",
    "ì•„ë˜ ì§€ì¹¨ì— ë”°ë¼ êµì•ˆ ë‚´ìš©ì„ **ê°„ê²°í•˜ê²Œ ìš”ì•½**í•˜ê³ , **ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ë‹¨ì›**ì„ ìˆœì°¨ì ìœ¼ë¡œ íŒë‹¨í•´ ì£¼ì„¸ìš”.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ” ë¶„ì„ ë‹¨ê³„ 1: êµì•ˆ ìš”ì•½\n",
    "- ì•„ë˜ êµì•ˆì„ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”:\n",
    "  - í•µì‹¬ ê°œë…: ìˆ˜í•™ì  ì›ë¦¬, ê°œë…, ë˜ëŠ” í™œë™\n",
    "  - í•™ìŠµ ëª©í‘œ: í•™ìƒì´ ì´ ìˆ˜ì—…ì„ í†µí•´ ë„ë‹¬í•´ì•¼ í•  ëª©í‘œ\n",
    "\n",
    "## ğŸ” ë¶„ì„ ë‹¨ê³„ 2: 1ì°¨ í›„ë³´ ë‹¨ì› ì„ ì • (ë‹¨ì›ëª… ê¸°ë°˜)\n",
    "- ì•„ë˜ ì œê³µëœ ì´ˆë“± ìˆ˜í•™ ë‹¨ì› ëª©ë¡ì„ ì°¸ê³ í•˜ì—¬, **êµì•ˆì˜ ì£¼ì œì™€ ë‹¨ì›ëª… ê°„ ìœ ì‚¬ë„**ë¥¼ íŒë‹¨í•´  \n",
    "  ê´€ë ¨ì„±ì´ ë†’ì€ ë‹¨ì› 5ê°œë¥¼ **ì •í™•í•œ ë‹¨ì›ëª…**ìœ¼ë¡œ ì¶”ë ¤ëƒ…ë‹ˆë‹¤.\n",
    "\n",
    "## ğŸ§  ë¶„ì„ ë‹¨ê³„ 3: ìµœì¢… ê´€ë ¨ ë‹¨ì› ì„ ì • (ë‚´ìš© ê¸°ë°˜)\n",
    "- ìœ„ì—ì„œ ì¶”ë¦° 5ê°œ ë‹¨ì›ì˜ í•µì‹¬ ê°œë…ê³¼ í•™ìŠµ ëª©í‘œë¥¼ í™•ì¸í•˜ê³ , **ìš”ì•½ëœ êµì•ˆ ë‚´ìš©ê³¼ ê°€ì¥ ë°€ì ‘í•œ ë‹¨ì› 1ê°œë¥¼ ìµœì¢… ì„ íƒ**í•©ë‹ˆë‹¤.\n",
    "- ë‹¨ìˆœí•œ í™œë™ ì¤‘ì‹¬ë³´ë‹¤ëŠ” ê°œë…ì„ ì„¤ëª…í•œ ë‹¨ì›ì´ ë” ì ì ˆí•©ë‹ˆë‹¤.\n",
    "- ë‹¨ì›ì˜ ëª…ì¹­ì€ ë°˜ë“œì‹œ **ì´ˆë“± ìˆ˜í•™ ë‹¨ì› ëª©ë¡ì—ì„œ ì¼ì¹˜í•˜ëŠ” ì´ë¦„ë§Œ ì‚¬ìš©**í•˜ì„¸ìš”.\n",
    "\n",
    "---\n",
    "\n",
    "âœ… ì£¼ì˜ì‚¬í•­:\n",
    "- ë‹¨ì›ëª… ì™¸ì—ëŠ” ë¶€ê°€ ì„¤ëª…, í•´ì„¤, ê´„í˜¸ ë“±ì„ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.\n",
    "- ì¶œë ¥ í˜•ì‹ì„ ë°˜ë“œì‹œ ì§€ì¼œ ì£¼ì„¸ìš”.\n",
    "\n",
    "---\n",
    "\n",
    "### Input (êµì•ˆ ì›ë¬¸):\n",
    "{text}\n",
    "\n",
    "---\n",
    "\n",
    "### ì´ˆë“± ìˆ˜í•™ ë‹¨ì› ëª©ë¡:\n",
    "{curriculum_units}\n",
    "\n",
    "---\n",
    "\n",
    "### Output í˜•ì‹:\n",
    "\n",
    "1. ìš”ì•½ ë‚´ìš©:\n",
    "- í•µì‹¬ ê°œë…: ...\n",
    "- í•™ìŠµ ëª©í‘œ: ...\n",
    "\n",
    "2. 1ì°¨ í›„ë³´ ë‹¨ì› (ë‹¨ì›ëª… ê¸°ë°˜ ìœ ì‚¬ë„ ìƒìœ„ 5ê°œ, JSON í˜•ì‹):\n",
    "```json\n",
    "{{\n",
    "  \"topic_1st\": \"ì—¬ê¸°ì— ë‹¨ì›ëª…\",\n",
    "  \"topic_2st\": \"ì—¬ê¸°ì— ë‹¨ì›ëª…\",\n",
    "  \"topic_3st\": \"ì—¬ê¸°ì— ë‹¨ì›ëª…\",\n",
    "  \"topic_4st\": \"ì—¬ê¸°ì— ë‹¨ì›ëª…\",\n",
    "  \"topic_5st\": \"ì—¬ê¸°ì— ë‹¨ì›ëª…\"\n",
    "}}\n",
    "\n",
    "3. ìµœì¢… ì„ íƒ ë‹¨ì› (ë‚´ìš© ê¸°ë°˜ ìœ ì‚¬ë„ ìµœìƒìœ„ 1ê°œ, JSON í˜•ì‹):\n",
    "```\n",
    "json\n",
    "{{\n",
    "  \"most_relevant_topic\": \"ì—¬ê¸°ì— ë‹¨ì›ëª…\"\n",
    "}}\n",
    "\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \n",
    "         \"content\": \"ë‹¹ì‹ ì€ ì´ˆë“±í•™êµ êµìœ¡ìë£Œë¥¼ ì˜ ìš”ì•½í•´ ì£¼ëŠ” ìœ ëŠ¥í•œ AIì…ë‹ˆë‹¤.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=True,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "    print(\"\\nğŸ§® ì…ë ¥ í† í° ìˆ˜:\", len(input_ids[0]))\n",
    "    \n",
    "    output_ids = llm_model.generate(\n",
    "        input_ids.to(\"cuda\"),\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        )\n",
    "\n",
    "    # âœ… ì—¬ê¸°ì„œ ë¬¸ìì—´ë¡œ ë””ì½”ë”©\n",
    "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # âœ… ë¬¸ìì—´ì— ëŒ€í•´ split ì ìš©\n",
    "    if \"[|assistant|]\" in output_text:\n",
    "        result = output_text.split(\"[|assistant|]\")[-1].strip()\n",
    "        print('\\n\\nâœ… ', result)\n",
    "        return result\n",
    "\n",
    "    # print('\\n\\n', output_text.strip())\n",
    "    return output_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70559131-1562-4e2c-ad58-fc00ce07a3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sentence_transformers import CrossEncoder\n",
    "\n",
    "# # CrossEncoder ëª¨ë¸ ë¡œë”© (í•œë²ˆë§Œ í•˜ë©´ ë¨)\n",
    "# cross_encoder = CrossEncoder(\"sentence-transformers/all-MiniLM-L12-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d51c03b-be80-4c35-8d6f-cf4a1ebfc17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_summary_for_rerank(summary_text):\n",
    "    print(\"âœ… í•¨ìˆ˜ ì‹œì‘\")\n",
    "\n",
    "    core_concept_match = re.search(\n",
    "        r\"^\\s*-\\s*\\**í•µì‹¬ ê°œë…\\**:\\s*(.+?)\\s*^\\s*-\\s*\\**í•™ìŠµ ëª©í‘œ\\**:\\s*(.+?)(?=\\n\\s*(?:\\d+\\.|\\Z))\",\n",
    "        summary_text,\n",
    "        re.DOTALL | re.MULTILINE,\n",
    "    )\n",
    "\n",
    "    if core_concept_match:\n",
    "        core_concept = core_concept_match.group(1).strip()\n",
    "        objective = core_concept_match.group(2).strip()\n",
    "        print(\"ğŸ” í•µì‹¬ê°œë…:\", core_concept)\n",
    "        print(\"ğŸ” í•™ìŠµëª©í‘œ:\", objective)\n",
    "    else:\n",
    "        print(\"âŒ í•µì‹¬ê°œë…/í•™ìŠµëª©í‘œ ì¶”ì¶œ ì‹¤íŒ¨\")\n",
    "        return []\n",
    "\n",
    "    llm_text = f\"í•µì‹¬ ê°œë…ì€ '{core_concept}'ì´ë©° í•™ìŠµ ëª©í‘œëŠ” '{objective}'ì´ë‹¤.\"\n",
    "\n",
    "    # 2) ê´€ë ¨ ë‹¨ì› JSON ì¶”ì¶œ (ì„¤ëª… ìˆëŠ” í•­ëª© ì œê±°)\n",
    "    candidate_titles = []\n",
    "    for i in range(1, 6):\n",
    "        pattern = rf'\"topic_{i}st\"\\s*:\\s*\"([^\"]+)\"'\n",
    "        match = re.search(pattern, summary_text)\n",
    "        if match:\n",
    "            title = match.group(1)\n",
    "            candidate_titles.append(title)\n",
    "\n",
    "    if not candidate_titles:\n",
    "        print(\"âŒ ê´€ë ¨ ë‹¨ì› ì¶”ì¶œ ì‹¤íŒ¨\")\n",
    "        return []\n",
    "\n",
    "    print(\"ğŸ¯ í›„ë³´ ë‹¨ì›:\", candidate_titles)\n",
    "\n",
    "    # 3) curriculum_unitsì—ì„œ title ê¸°ë°˜ìœ¼ë¡œ ë¬¸ì¥ìŒ ë§Œë“¤ê¸°\n",
    "    title2item = {item[\"title\"]: item for item in curriculum_units}\n",
    "    sentence_pairs = []\n",
    "    valid_candidates = []\n",
    "\n",
    "    print(\"llm_text: \", llm_text)\n",
    "    for title in candidate_titles:\n",
    "        item = title2item.get(title)\n",
    "        if item:\n",
    "            # candidate_text = f\"{item['title']} {item['core_concept']} {item['objective']}\"\n",
    "            candidate_text = f\"{item['title']} ë‹¨ì›ì˜ í•µì‹¬ ê°œë…ì€ '{item['core_concept']}' ì´ë©°, í•™ìŠµ ëª©í‘œëŠ” '{item['objective']}' ì´ë‹¤.\"\n",
    "            print(\"candidate_text: \", candidate_text)\n",
    "            sentence_pairs.append((llm_text, candidate_text))\n",
    "            valid_candidates.append(title)\n",
    "        else:\n",
    "            print(f\"ğŸš« curriculum_unitsì— ì—†ëŠ” ë‹¨ì› íŒ¨ìŠ¤: {title}\")\n",
    "\n",
    "    if not sentence_pairs:\n",
    "        print(\"âŒ ìœ íš¨í•œ ì†Œë‹¨ì› ì—†ìŒ\")\n",
    "        return []\n",
    "\n",
    "    # 4) CrossEncoder ì˜ˆì¸¡ ë° ì •ë ¬\n",
    "    scores = cross_encoder.predict(sentence_pairs)\n",
    "    scored = sorted(zip(valid_candidates, scores), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    print(\"âœ… ìµœì¢… ì„ ì •ëœ ì†Œë‹¨ì›:\", scored)\n",
    "\n",
    "    return scored  # List of (title, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835a68e4-d42b-457c-b5b4-9d1de95efbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_valid_topics(summary_text: str) -> list[str]:\n",
    "    # curriculum_unitsì— ì¡´ì¬í•˜ëŠ” title ì§‘í•©\n",
    "    subtopic_titles = {item[\"title\"] for item in curriculum_units}\n",
    "    \n",
    "    valid_topics = []\n",
    "\n",
    "    # ìµœëŒ€ 5ê°œì˜ í›„ë³´ ì£¼ì œ ì¶”ì¶œ\n",
    "    for i in range(1, 6):\n",
    "        pattern = rf'\"topic_{i}st\"\\s*:\\s*\"([^\"]+)\"'\n",
    "        match = re.search(pattern, summary_text)\n",
    "        if match:\n",
    "            title = match.group(1)\n",
    "            if title in subtopic_titles:\n",
    "                print(f\"âœ… ìœ íš¨í•œ ì£¼ì œ ë°œê²¬: {title}\")\n",
    "                valid_topics.append(title)\n",
    "    \n",
    "    return valid_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8680ec-e6d6-49fb-9ad2-e39352b02650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_exaone_select(summary_text: str) -> str:\n",
    "    # 1. ìš”ì•½ë¬¸ì—ì„œ í•µì‹¬ ê°œë…ê³¼ í•™ìŠµ ëª©í‘œ ì¶”ì¶œ (ê°ê° ë”°ë¡œ)\n",
    "    core_concept_match = re.search(\n",
    "        r\"-\\s*\\*?\\*?í•µì‹¬ ê°œë…\\*?\\*?\\s*:\\s*(.+?)(?=\\n\\s*-\\s*\\*?\\*?í•™ìŠµ ëª©í‘œ\\*?\\*?\\s*:)\",\n",
    "        summary_text,\n",
    "        re.DOTALL,\n",
    "    )\n",
    "\n",
    "    objective_match = re.search(\n",
    "        r\"-\\s*\\*?\\*?í•™ìŠµ ëª©í‘œ\\*?\\*?\\s*:\\s*(.+?)(?=\\n\\s*\\d+\\.|\\n\\s*```|\\Z)\",\n",
    "        summary_text,\n",
    "        re.DOTALL,\n",
    "    )\n",
    "\n",
    "    if not core_concept_match or not objective_match:\n",
    "        print(\"âŒ í•µì‹¬ ê°œë…/í•™ìŠµ ëª©í‘œ ì¶”ì¶œ ì‹¤íŒ¨\")\n",
    "        return \"\"\n",
    "\n",
    "    core_concept = core_concept_match.group(1).strip()\n",
    "    objective = objective_match.group(1).strip()\n",
    "\n",
    "    # 2. topic_1st ~ topic_5st ì—ì„œ í›„ë³´ ë‹¨ì› ì œëª© ì¶”ì¶œ\n",
    "    candidate_titles = []\n",
    "    for i in range(1, 6):\n",
    "        pattern = rf'\"topic_{i}st\"\\s*:\\s*\"([^\"]+)\"'\n",
    "        match = re.search(pattern, summary_text)\n",
    "        if match:\n",
    "            candidate_titles.append(match.group(1))\n",
    "\n",
    "    if not candidate_titles:\n",
    "        print(\"âŒ í›„ë³´ ë‹¨ì› ì¶”ì¶œ ì‹¤íŒ¨\")\n",
    "        return \"\"\n",
    "\n",
    "    # 3. í›„ë³´ ì¤‘ curriculum_unitsì— ì¡´ì¬í•˜ëŠ” ìœ íš¨í•œ ë‹¨ì› í•„í„°ë§\n",
    "    title2item = {item[\"title\"]: item for item in curriculum_units}\n",
    "    valid_candidates = []\n",
    "    candidate_descriptions = []\n",
    "\n",
    "    for title in candidate_titles:\n",
    "        item = title2item.get(title)\n",
    "        if item:\n",
    "            description = (\n",
    "                f\"{item['title']}\\n\"\n",
    "                f\"í•µì‹¬ ê°œë…: {item['core_concept']}\\n\"\n",
    "                f\"í•™ìŠµ ëª©í‘œ: {item['objective']}\\n\"\n",
    "            )\n",
    "            valid_candidates.append(item[\"title\"])\n",
    "            candidate_descriptions.append(description)\n",
    "        else:\n",
    "            print(f\"ğŸš« curriculum_unitsì— ì—†ëŠ” ë‹¨ì› íŒ¨ìŠ¤: {title}\")\n",
    "\n",
    "    if not valid_candidates:\n",
    "        print(\"âŒ ìœ íš¨í•œ ë‹¨ì›ì´ ì—†ìŠµë‹ˆë‹¤\")\n",
    "        return \"\"\n",
    "\n",
    "    # 4. í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
    "    curriculum_titles_text = \"\\n\".join(f\"- {desc}\" for desc in candidate_descriptions)\n",
    "\n",
    "    prompt = f\"\"\"ë‹¤ìŒì€ ì´ˆë“± ìˆ˜í•™ ìˆ˜ì—… ìš”ì•½ì…ë‹ˆë‹¤. ì„¤ëª…ê³¼ **ê°€ì¥ ê´€ë ¨ ìˆëŠ” ê°œë… ì¤‘ì‹¬ ë‹¨ì› í•˜ë‚˜ë§Œ** ì •í™•í•˜ê²Œ ê³¨ë¼ì£¼ì„¸ìš”.\n",
    "\n",
    "- ë°˜ë“œì‹œ ì•„ë˜ ëª©ë¡ì—ì„œ **í•˜ë‚˜ë§Œ ì„ íƒ**í•´ì•¼ í•˜ë©°, **ì •í™•í•œ ë‹¨ì›ëª…ë§Œ** ì‘ì„±í•´ ì£¼ì„¸ìš”. (ë¶€ì—° ì„¤ëª…, ê´„í˜¸, ì¤„ë°”ê¿ˆ ì—†ì´ ë‹¨ìˆœíˆ ë‹¨ì›ëª…ë§Œ!)\n",
    "- **ê°œë…ì´ë‚˜ ì •ì˜ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ í•œ ë‹¨ì›**ì„ ìš°ì„ ì ìœ¼ë¡œ ê³ ë ¤í•´ ì£¼ì„¸ìš”. ë‹¨ìˆœí•œ í™œë™ ì¤‘ì‹¬ë³´ë‹¤ëŠ” ê°œë…ì„ ì„¤ëª…í•œ ë‹¨ì›ì´ ë” ì ì ˆí•©ë‹ˆë‹¤.\n",
    "- ì•„ë˜ì™€ ê°™ì€ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•´ ì£¼ì„¸ìš”.\n",
    "\n",
    "- **í•µì‹¬ ê°œë…**: {core_concept}\n",
    "- **í•™ìŠµ ëª©í‘œ**: {objective}\n",
    "\n",
    "ë‹¤ìŒì€ ì†Œë‹¨ì› ëª©ë¡ì…ë‹ˆë‹¤:\n",
    "{curriculum_titles_text}\n",
    "\n",
    "ì‘ë‹µ ì˜ˆì‹œ:\n",
    "```json\n",
    "{{\n",
    "  \"valid_topic\": \"ì—¬ê¸°ì— ì •í™•í•œ ë‹¨ì›ëª…ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”\"\n",
    "}}\n",
    "```\"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \n",
    "         \"content\": \"ë‹¹ì‹ ì€ ì´ˆë“±í•™êµ ìˆ˜í•™ êµìœ¡ ë‚´ìš©ì„ ë¶„ì„í•´ ê°€ì¥ ê´€ë ¨ ìˆëŠ” ë‹¨ì›ì„ ì¶”ì²œí•˜ëŠ” ì „ë¬¸ê°€ AIì…ë‹ˆë‹¤.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    print(\"\\nğŸ§® ì…ë ¥ í† í° ìˆ˜:\", len(input_ids[0]))\n",
    "\n",
    "    output_ids = llm_model.generate(\n",
    "        input_ids.to(\"cuda\"),\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=False,\n",
    "    )\n",
    "\n",
    "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    # print(\"ğŸ“ LLM ì¶œë ¥:\\n\", output_text)\n",
    "     \n",
    "    matches = re.findall(r'\"valid_topic\"\\s*:\\s*\"([^\"]+)\"', output_text)\n",
    "    \n",
    "    if matches:\n",
    "        selected_title = matches[-1].strip()\n",
    "        # print(\"valid_candidates: \", valid_candidates)\n",
    "    \n",
    "        if selected_title in [v.strip() for v in valid_candidates]:\n",
    "            print(\"âœ… ìµœì¢… ì¶”ì²œ ë‹¨ì›:\", selected_title)\n",
    "            return selected_title\n",
    "        else:\n",
    "            print(f\"âš ï¸ ì¶”ì²œëœ ë‹¨ì›ì´ í›„ë³´êµ°ì— ì—†ìŒ: {selected_title}\")\n",
    "    else:\n",
    "        print(\"âŒ ì¶œë ¥ì—ì„œ valid_topic ì¶”ì¶œ ì‹¤íŒ¨\")\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144c2af7-a333-4694-b202-771832a2a81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ocr_merge_node(state: dict) -> dict:\n",
    "    easy = state[\"easyocr_result\"]\n",
    "    paddle = state[\"paddleocr_result\"]\n",
    "    result = run_ocr_merge(easy, paddle)\n",
    "\n",
    "    output = {\n",
    "        \"merged_text\": result[\"merged_text\"],\n",
    "        \"is_math_related\": result[\"is_math_related\"]\n",
    "    }\n",
    "\n",
    "    # warningì´ ìˆëŠ” ê²½ìš°ì—ë§Œ í¬í•¨\n",
    "    if \"warning\" in result:\n",
    "        output[\"warning\"] = result[\"warning\"]\n",
    "        \n",
    "    return output\n",
    "\n",
    "def summarize_node(state: dict) -> dict:\n",
    "    input_text = state[\"merged_text\"]\n",
    "    summary = run_exaone_summary(input_text)\n",
    "    # parse_summary_for_rerank(summary)\n",
    "    # valid_tipics = extract_all_valid_topics(summary)\n",
    "    return {\"summary\": summary}\n",
    "\n",
    "def select_topic_node(state: dict) -> dict:\n",
    "    summary = state[\"summary\"]\n",
    "    valid_topic = run_exaone_select(summary)\n",
    "    return {\"valid_topic\": valid_topic}\n",
    "\n",
    "def end_with_warning_node(state: dict) -> dict:\n",
    "    print(state.get(\"warning\", \"âš ï¸ ë¹„ì •ìƒ ì¢…ë£Œ\"))\n",
    "    return {}  # ê²°ê³¼ ë°˜í™˜ ì—†ì´ ì¢…ë£Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d72de98-e6e3-4eb6-a255-3487724bff5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(AgentState)\n",
    "builder.add_node(\"ocr_merge\", RunnableLambda(ocr_merge_node)) # OCR ê²°ê³¼(EasyOCRì™€ PaddleOCR)ë¥¼ í†µí•©\n",
    "builder.add_node(\"summarize\", RunnableLambda(summarize_node))\n",
    "builder.add_node(\"select_topic\", RunnableLambda(select_topic_node))\n",
    "builder.add_node(\"end_with_warning\", RunnableLambda(end_with_warning_node))\n",
    "\n",
    "builder.set_entry_point(\"ocr_merge\")\n",
    "\n",
    "# ìˆ˜í•™ ê´€ë ¨ ì—¬ë¶€ì— ë”°ë¼ íë¦„ ê²°ì •\n",
    "builder.add_conditional_edges(\n",
    "    \"ocr_merge\",\n",
    "    lambda state: \"end_with_warning\" if not state.get(\"is_math_related\", False) else \"summarize\"\n",
    ")\n",
    "\n",
    "builder.add_edge(\"summarize\", \"select_topic\")\n",
    "builder.add_edge(\"select_topic\", END)\n",
    "builder.add_edge(\"end_with_warning\", END)\n",
    "\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23424d4-560c-46e0-92bf-3e4bf01c148e",
   "metadata": {},
   "source": [
    "### OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b81597-062e-41d3-8254-9af674cbd35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) EasyOCR reader ìƒì„± (í•œê¸€+ì˜ì–´)\n",
    "reader = easyocr.Reader(['ko', 'en'], gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490fd3ac-22de-4f2b-b846-a8289ddc17a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EasyOCR_from_file(file_path=None, url=None, show_image=True):\n",
    "    if file_path:\n",
    "        img = cv2.imread(file_path)\n",
    "    elif url:\n",
    "        response  = requests.get(url)\n",
    "        img = np.asarray(bytearray(response.content), dtype=\"uint8\")\n",
    "        img = cv2.imdecode(img, cv2.IMREAD_COLOR)\n",
    "    else:\n",
    "        raise ValueError(\"Either file_path or url must be provided.\")\n",
    "\n",
    "    # BGR(OpenCV) -> RGB(PIL) ë³€í™˜\n",
    "    img = cv2.resize(img, None, fx=2, fy=2, interpolation=cv2.INTER_CUBIC)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    blur = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "    _, binary = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "    # 4) ì´ë¯¸ì§€ì— OCR ìˆ˜í–‰\n",
    "    results = reader.readtext(binary)\n",
    "    \n",
    "    # 5) ê²°ê³¼ ì¶œë ¥\n",
    "    # for bbox, text, prob in results:\n",
    "    #     print(f'Text: {text}, Confidence: {prob:.2f}')\n",
    "\n",
    "    # ì´ë¯¸ì§€ ì¶œë ¥\n",
    "    if show_image:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(binary, cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    \n",
    "    # merged_text = \" \".join([text for _, text, _ in results])\n",
    "    return results, binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1590c9ba-c80d-4660-adb6-43c3486c6e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PaddleOCRì„ thread ë°©ì‹ì´ ì•„ë‹Œ ì§ì ‘ í˜¸ì¶œ ë°©ì‹ìœ¼ë¡œ ì‚¬ìš© ì‹œ instance ìƒì„± í•„ìš”\n",
    "ocr = PaddleOCR(text_recognition_model_name=\"korean_PP-OCRv5_mobile_rec\",\n",
    "                use_doc_orientation_classify=False,\n",
    "                use_doc_unwarping=False,\n",
    "                use_textline_orientation=True,  # í…ìŠ¤íŠ¸ ë°©í–¥ ë³´ì •\n",
    "                text_det_box_thresh=0.4,        # ë°•ìŠ¤ ì„ê³„ê°’\n",
    "                text_rec_score_thresh=0.6,      # ì¸ì‹ ì‹ ë¢°ë„ ì„ê³„ê°’\n",
    "                text_det_unclip_ratio=1.5,      # í…ìŠ¤íŠ¸ ë°•ìŠ¤ í™•ì¥ ë¹„ìœ¨\n",
    "                text_det_thresh=0.3,             # í…ìŠ¤íŠ¸ ê°ì§€ ì„ê³„ê°’\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e21b8b-6f3a-4a8e-b2b3-7476668b0498",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PaddleOCR_from_file(file_path=None, url=None, show_image=True):\n",
    "    if file_path:\n",
    "        img = cv2.imread(file_path)\n",
    "    elif url:\n",
    "        response = requests.get(url)\n",
    "        img = np.asarray(bytearray(response.content), dtype=\"uint8\")\n",
    "        img = cv2.imdecode(img, cv2.IMREAD_COLOR)\n",
    "    else:\n",
    "        raise ValueError(\"Either file_path or url must be provided.\")\n",
    "\n",
    "    # ì´ë¯¸ì§€ ì „ì²˜ë¦¬\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    denoised = cv2.fastNlMeansDenoising(gray)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    enhanced = clahe.apply(denoised)\n",
    "    _, binary = cv2.threshold(enhanced, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    padding = 30\n",
    "    padded_binary = cv2.copyMakeBorder(binary, padding, padding, padding, padding, cv2.BORDER_CONSTANT, value=255)\n",
    "    color_binary = cv2.cvtColor(padded_binary, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    # OCR ìˆ˜í–‰\n",
    "    result = ocr.predict(color_binary)\n",
    "    ocr_result = result[0]\n",
    "\n",
    "    texts = ocr_result[\"rec_texts\"]        # í…ìŠ¤íŠ¸ ì¸ì‹ ê²°ê³¼\n",
    "    scores = ocr_result[\"rec_scores\"]      # ì¸ì‹ ì‹ ë¢°ë„\n",
    "    polys  = ocr_result[\"rec_polys\"]       # í…ìŠ¤íŠ¸ ìœ„ì¹˜ (polygon)\n",
    "\n",
    "    # ì´ë¯¸ì§€ ì¶œë ¥\n",
    "    if show_image:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(color_binary)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    return ' '.join(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e8c38b-ee03-45d3-8275-94266d94cafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PaddleOCRì„ thread ë°©ì‹ìœ¼ë¡œ ì‚¬ìš©\n",
    "paddleocr_result = None\n",
    "\n",
    "def PaddleOCR_from_file_thread(file_path=None, url=None, show_image=True):\n",
    "    if file_path:\n",
    "        img = cv2.imread(file_path)\n",
    "    elif url:\n",
    "        response = requests.get(url)\n",
    "        img = np.asarray(bytearray(response.content), dtype=\"uint8\")\n",
    "        img = cv2.imdecode(img, cv2.IMREAD_COLOR)\n",
    "    else:\n",
    "        raise ValueError(\"Either file_path or url must be provided.\")\n",
    "\n",
    "    ocr = PaddleOCR(text_recognition_model_name=\"korean_PP-OCRv5_mobile_rec\",\n",
    "                    use_doc_orientation_classify=False,\n",
    "                    use_doc_unwarping=False,\n",
    "                    use_textline_orientation=True,  # í…ìŠ¤íŠ¸ ë°©í–¥ ë³´ì •\n",
    "                    text_det_box_thresh=0.4,        # ë°•ìŠ¤ ì„ê³„ê°’\n",
    "                    text_rec_score_thresh=0.6,      # ì¸ì‹ ì‹ ë¢°ë„ ì„ê³„ê°’\n",
    "                    text_det_unclip_ratio=1.5,      # í…ìŠ¤íŠ¸ ë°•ìŠ¤ í™•ì¥ ë¹„ìœ¨\n",
    "                    text_det_thresh=0.3,             # í…ìŠ¤íŠ¸ ê°ì§€ ì„ê³„ê°’\n",
    "                   )#use_angle_cls\n",
    "\n",
    "    height, width = img.shape[:2]\n",
    "    print(f\"Width: {width}, Height: {height}\")\n",
    "\n",
    "    # ì´ë¯¸ì§€ ì „ì²˜ë¦¬\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    denoised = cv2.fastNlMeansDenoising(gray)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    enhanced = clahe.apply(denoised)\n",
    "    _, binary = cv2.threshold(enhanced, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    padding = 30\n",
    "    padded_binary = cv2.copyMakeBorder(binary, padding, padding, padding, padding, cv2.BORDER_CONSTANT, value=255)\n",
    "    color_binary = cv2.cvtColor(padded_binary, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    # OCR ìˆ˜í–‰\n",
    "    result = ocr.predict(color_binary)\n",
    "    ocr_result = result[0]\n",
    "\n",
    "    texts = ocr_result[\"rec_texts\"]        # í…ìŠ¤íŠ¸ ì¸ì‹ ê²°ê³¼\n",
    "    scores = ocr_result[\"rec_scores\"]      # ì¸ì‹ ì‹ ë¢°ë„\n",
    "    polys  = ocr_result[\"rec_polys\"]       # í…ìŠ¤íŠ¸ ìœ„ì¹˜ (polygon)\n",
    "\n",
    "    # ì´ë¯¸ì§€ ì¶œë ¥\n",
    "    if show_image:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(color_binary)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    global paddleocr_result\n",
    "    paddleocr_result = ' '.join(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce3d69e-a4c7-4774-a2c9-3682d65fcfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ì‚¬ì „ ì¤€ë¹„: TrOCR ë¡œë”©\n",
    "# trocr_processor = TrOCRProcessor.from_pretrained(\"ddobokki/ko-trocr\") \n",
    "# trocr_model = VisionEncoderDecoderModel.from_pretrained(\"ddobokki/ko-trocr\")\n",
    "# trocr_tokenizer = AutoTokenizer.from_pretrained(\"ddobokki/ko-trocr\")\n",
    "\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# trocr_model = trocr_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d432fa-43b4-4c57-a939-9f83bbeaa5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def TROCR_from_file(detection_result, image):\n",
    "#     img_pil = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_GRAY2RGB))\n",
    "#     ocr_texts = []\n",
    "#     for (bbox, _, _) in detection_result:\n",
    "#         # bboxëŠ” 4ê°œì˜ ì  ì¢Œí‘œë¡œ ì£¼ì–´ì§ -> xmin, ymin, xmax, ymaxë¡œ ë³€í™˜\n",
    "#         pts = np.array(bbox).astype(int)\n",
    "#         xmin = np.min(pts[:, 0])\n",
    "#         ymin = np.min(pts[:, 1])\n",
    "#         xmax = np.max(pts[:, 0])\n",
    "#         ymax = np.max(pts[:, 1])\n",
    "    \n",
    "#         # ë°•ìŠ¤ ì˜ì—­ ì˜ë¼ë‚´ê¸°\n",
    "#         cropped = img_pil.crop((xmin, ymin, xmax, ymax))\n",
    "    \n",
    "#         # TrOCR ì¶”ë¡ \n",
    "#         pixel_values = trocr_processor(cropped, return_tensors=\"pt\").pixel_values.to(device)\n",
    "#         generated_ids = trocr_model.generate(pixel_values, max_length=64)\n",
    "#         generated_text = trocr_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "#         generated_text = unicodedata.normalize(\"NFC\", generated_text)\n",
    "#         ocr_texts.append(generated_text)\n",
    "#         # print(\"Detected text:\", generated_text)\n",
    "\n",
    "#     return ' '.join(ocr_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63714fcd-b4d0-432a-a172-08e8025b9498",
   "metadata": {},
   "source": [
    "### TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b27156-af38-4db9-b069-bac7d498d54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47bc49e-8d69-4783-9701-d50a9d6361bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "url = 'https://www.home-learn.co.kr/common/image.do?imgPath=newsroom&imgName=CK20221226131232394.png&imgGubun=D'\n",
    "file_path = ''\n",
    "paddle_thread = threading.Thread(target=PaddleOCR_from_file_thread, args=(file_path, url, False))\n",
    "paddle_thread.start()\n",
    "\n",
    "results, binary = EasyOCR_from_file(file_path, url)\n",
    "easyocr_result = \" \".join([text for _, text, _ in results])\n",
    "print(\"EasyOCR ê²°ê³¼: \", easyocr_result)\n",
    "\n",
    "# trocr_result = TROCR_from_file(results, binary)\n",
    "# print(\"\\nTROCR ê²°ê³¼: \", trocr_result)\n",
    "\n",
    "paddle_thread.join()\n",
    "\n",
    "# paddleocr_result = PaddleOCR_from_file(url=url, show_image=False)\n",
    "print(\"\\nPaddleOCR ê²°ê³¼: \", paddleocr_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f83e213-1277-4b6b-8f4a-a43343963fae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "result = graph.invoke({\"easyocr_result\": easyocr_result, \"paddleocr_result\":paddleocr_result})\n",
    "print(\"âœ… ìš”ì•½ ë° ë¬¸ì œ ìƒì„± ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c9825a-b006-4bb4-871e-b8d3443a7811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_graph(file_path=None, url=None):\n",
    "    paddle_thread = threading.Thread(target=PaddleOCR_from_file_thread, args=(file_path, url, False))\n",
    "    paddle_thread.start()\n",
    "    \n",
    "    results, binary = EasyOCR_from_file(file_path, url)\n",
    "    easyocr_result = \" \".join([text for _, text, _ in results])\n",
    "    print(\"EasyOCR ê²°ê³¼: \", easyocr_result)\n",
    "    \n",
    "    # trocr_result = TROCR_from_file(results, binary)\n",
    "    # print(\"\\nTROCR ê²°ê³¼: \", trocr_result)\n",
    "    \n",
    "    paddle_thread.join()\n",
    "    \n",
    "    # paddleocr_result = PaddleOCR_from_file(url=url, show_image=False)\n",
    "    print(\"\\nPaddleOCR ê²°ê³¼: \", paddleocr_result)\n",
    "    \n",
    "    result = graph.invoke({\"easyocr_result\": easyocr_result, \"paddleocr_result\":paddleocr_result})\n",
    "    print(\"âœ… ìš”ì•½ ë° ë¬¸ì œ ìƒì„± ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e435554-be2c-4119-af86-948882fb674a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "url = 'https://www.home-learn.co.kr/common/image.do?imgPath=newsroom&imgName=CK20221222103639635.png&imgGubun=D'\n",
    "test_graph(url=url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f8971f-11f2-4b2d-bdfa-863877880e75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "url = 'https://www.home-learn.co.kr/common/image.do?imgPath=newsroom&imgName=CK20221222103435065.png&imgGubun=D'\n",
    "test_graph(url=url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511f806b-a447-45f2-923a-66f31a43a207",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "url = 'https://www.home-learn.co.kr/common/image.do?imgPath=newsroom&imgName=CK20230428170405246.png&imgGubun=D'\n",
    "test_graph(url=url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c459c92f-1cf8-4917-8321-b8bbf87de416",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "url = 'https://www.home-learn.co.kr/common/image.do?imgPath=newsroom&imgName=CK20221226131232394.png&imgGubun=D'\n",
    "test_graph(url=url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57efec0-bafc-4b52-811b-067c37ccf505",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "url = 'https://www.home-learn.co.kr/common/image.do?imgPath=newsroom&imgName=CK20221222104929447.png&imgGubun=D'\n",
    "test_graph(url=url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e293d7b-067b-48ee-a93e-c2db87127608",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "url = 'https://www.home-learn.co.kr/common/image.do?imgPath=newsroom&imgName=CK20221226131610654.png&imgGubun=D'\n",
    "test_graph(url=url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d638099f-62b9-43ba-b229-9c0b271960d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "url = 'https://www.home-learn.co.kr/common/image.do?imgPath=newsroom&imgName=CK20221226131632718.png&imgGubun=D'\n",
    "test_graph(url=url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2362c017-29f6-4b26-8720-33a69541969a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "url = 'https://www.home-learn.co.kr/common/image.do?imgPath=newsroom&imgName=CK20221226132234942.png&imgGubun=D'\n",
    "test_graph(url=url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537e4b00-86d4-4cf6-800c-29ce631c5eaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "url = 'https://www.home-learn.co.kr/common/image.do?imgPath=newsroom&imgName=CK20221226132145911.png&imgGubun=D'\n",
    "test_graph(url=url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f202a7e-e61e-43e8-91bf-a956a2d72e8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "url = 'https://www.home-learn.co.kr/common/image.do?imgPath=newsroom&imgName=CK20221222104708761.png&imgGubun=D'\n",
    "test_graph(url=url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c6ca53-fbc0-4eee-8c02-600c089dc027",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "url = 'https://www.home-learn.co.kr/common/image.do?imgPath=newsroom&imgName=CK20221222104717403.png&imgGubun=D'\n",
    "test_graph(url=url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f31b8ee-144f-4aa3-a3b8-4ec5beff86af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "url = 'https://www.home-learn.co.kr/common/image.do?imgPath=newsroom&imgName=CK20221229134742525.png&imgGubun=D'\n",
    "test_graph(url=url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3048b8e5-1cb1-4685-a393-446969e11dcc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "url = 'https://www.home-learn.co.kr/common/image.do?imgPath=newsroom&imgName=CK20221229135232326.png&imgGubun=D'\n",
    "test_graph(url=url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5d5451-9aad-41f0-bcda-484bc3a3bf00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "url = 'https://www.home-learn.co.kr/common/image.do?imgPath=newsroom&imgName=CK20230425125336384.jpeg&imgGubun=D'\n",
    "test_graph(url=url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7686d73b-1cbc-4744-8721-f07b091d3893",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "url = 'https://www.home-learn.co.kr/common/image.do?imgPath=newsroom&imgName=CK20230102143620114.png&imgGubun=D'\n",
    "test_graph(url=url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6a3123-9e99-4abb-9ffc-9099fe8f29b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "url = 'https://www.home-learn.co.kr/common/image.do?imgPath=newsroom&imgName=CK20230102143842371.png&imgGubun=D'\n",
    "test_graph(url=url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f283373-f04e-48fd-959b-a22f2106857c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "url = 'https://www.home-learn.co.kr/common/image.do?imgPath=newsroom&imgName=CK20221226131332329.png&imgGubun=D'\n",
    "test_graph(url=url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529739ce-8666-44aa-bd63-f0232cda90a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "file_path = 'Office_Lens_20161216-122020.jpg'\n",
    "test_graph(file_path=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2bb1f0-e7fd-4952-89e4-8618e83d85f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "url = 'https://i.ytimg.com/vi/55d4n_dQxs4/maxresdefault.jpg?'\n",
    "test_graph(url=url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24227b71-7fc2-4e81-b8e9-ca007680ed2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "file_path = 'ratio.png'\n",
    "test_graph(file_path=file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e6b1c4-fe43-4dd0-a644-e1a3059fb959",
   "metadata": {},
   "source": [
    "### EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df38185-7de4-4d3d-9009-983743b5baf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_prompt = f\"\"\"\n",
    "ë‹¤ìŒì€ ë‘ ê°œì˜ OCR ê²°ê³¼(EasyOCR, PaddleOCR)ë¥¼ í†µí•©í•˜ì—¬ LLMì´ ìƒì„±í•œ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤.  \n",
    "ì´ í†µí•© ê²°ê³¼ê°€ ì›ë¬¸ì— ê¸°ë°˜í•˜ì—¬ ì–¼ë§ˆë‚˜ ì •í™•í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ì§€ë¥¼ ì•„ë˜ ê¸°ì¤€ì— ë”°ë¼ í‰ê°€í•´ ì£¼ì„¸ìš”.\n",
    "\n",
    "## í‰ê°€ í•­ëª©:\n",
    "1. **ì •í™•ì„± (0~10ì )**: ë‘ OCR ê²°ê³¼ì— ìˆëŠ” ì •ë³´ë¥¼ ì¶©ì‹¤íˆ ë°˜ì˜í–ˆëŠ”ê°€? ëˆ„ë½, ì™œê³¡ ì—†ì´ ì˜ í†µí•©í–ˆëŠ”ê°€?\n",
    "2. **ì˜¤ë¥˜ ìˆ˜ì • (0~10ì )**: ì¸ì‹ ì˜¤ë¥˜(ì˜¤íƒˆì, ì˜ëª»ëœ ë‹¨ì–´ ë“±)ë¥¼ ë¬¸ë§¥ì— ë§ê²Œ ìì—°ìŠ¤ëŸ½ê²Œ ìˆ˜ì •í–ˆëŠ”ê°€?\n",
    "3. **ë¬¸ì¥ì˜ ìì—°ìŠ¤ëŸ¬ì›€ (0~10ì )**: ê²°ê³¼ ë¬¸ì¥ì´ ë¬¸ë²•ì ìœ¼ë¡œ ë§¤ë„ëŸ½ê³  ì½ê¸° ì‰¬ìš´ê°€?\n",
    "\n",
    "ê° í•­ëª©ì„ 0~10ì ìœ¼ë¡œ í‰ê°€í•œ í›„, ì „ì²´ í‰ê·  ì ìˆ˜(ì†Œìˆ˜ì  ì²«ì§¸ ìë¦¬ê¹Œì§€)ë¥¼ **\"total_score\"**ë¡œ ê³„ì‚°í•´ ì£¼ì„¸ìš”.  \n",
    "\n",
    "## ì¶œë ¥ í˜•ì‹ (JSON):\n",
    "```json\n",
    "{{\n",
    "  \"evaluation\": {{\n",
    "    \"accuracy\": 8.5,\n",
    "    \"error_correction\": 9.0,\n",
    "    \"fluency\": 9.5,\n",
    "    \"total_score\": 9.0\n",
    "  }},\n",
    "  \"comments\": \"ëŒ€ë¶€ë¶„ì˜ ì •ë³´ë¥¼ ì˜ í†µí•©í–ˆìœ¼ë©°, ì˜¤íƒˆì ìˆ˜ì •ë„ í›Œë¥­í•©ë‹ˆë‹¤. ë‹¤ë§Œ ì¼ë¶€ ìˆ«ì ì¸ì‹ ì˜¤ë¥˜ê°€ ê·¸ëŒ€ë¡œ ìœ ì§€ë˜ì–´ ê°ì ë˜ì—ˆìŠµë‹ˆë‹¤.\"\n",
    "}}\n",
    "\n",
    "## OCR ê²°ê³¼:\n",
    "### EasyOCR:\n",
    "{easyocr_result}\n",
    "\n",
    "### PaddleOCR:\n",
    "{paddleocr_result}\n",
    "\n",
    "## LLM í†µí•© ê²°ê³¼:\n",
    "{result[\"merged_text\"]}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b130a85e-6f49-4b32-a572-f99a95be883c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„±\n",
    "client = OpenAI(api_key=\"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\")  # ì‹¤ì œ í‚¤ë¡œ ëŒ€ì²´\n",
    "\n",
    "# GPT-4o-mini í˜¸ì¶œ\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"ë‹¹ì‹ ì€ OCR ê²°ê³¼ í†µí•©ì„ ì „ë¬¸ìœ¼ë¡œ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‘ ê°€ì§€ OCR ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ ìì—°ìŠ¤ëŸ½ê³  ì •í™•í•œ ë¬¸ì¥ìœ¼ë¡œ í†µí•©í•œ \n",
    "                        AIì˜ ê²°ê³¼ë¬¼ì„ í‰ê°€í•˜ëŠ” ì—­í• ì…ë‹ˆë‹¤. í‰ê°€ ê¸°ì¤€ì€ ì •í™•ì„±, ì˜¤ë¥˜ ìˆ˜ì • ëŠ¥ë ¥, ë¬¸ì¥ì˜ ìì—°ìŠ¤ëŸ¬ì›€ì…ë‹ˆë‹¤. ì´ ê¸°ì¤€ì— ë”°ë¼ ì„¸ì‹¬í•˜ê²Œ \n",
    "                        í‰ê°€í•˜ê³  ê·¼ê±°ë¥¼ ë“¤ì–´ ì„¤ëª…í•´ ì£¼ì„¸ìš”.\"\"\"\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": evaluation_prompt}\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    max_tokens=256\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c83e4a1-e871-4a0d-adc1-d2a553c02207",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68247169-1332-4eaf-95a5-418ed1763efb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cc09a7-e733-42a9-b751-e46f77546b7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a00008-a640-40cc-b628-327ad0de9323",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
